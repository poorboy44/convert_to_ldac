{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## converts tweets to LDAC format\n",
    "from collections import defaultdict\n",
    "import codecs\n",
    "fin='sample_tweets.txt'\n",
    "fout=fin+'.ldac'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#word count\n",
    "token_set = defaultdict(int)\n",
    "with codecs.open(fin, 'rb', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        tokens=line.strip().lower().split(' ')\n",
    "        for token in tokens:\n",
    "            token_set[token]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18233\n",
      "5191\n"
     ]
    }
   ],
   "source": [
    "#delete terms with only 1 occurence\n",
    "token_set2={k: v for k, v in token_set.iteritems() if v>10}\n",
    "print(len(token_set))\n",
    "print(len(token_set2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create the vocab\n",
    "vocab={k:i for i, k in enumerate(token_set2.keys())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create the DTM\n",
    "line_num=0\n",
    "vocab_set=set(vocab.keys())\n",
    "with codecs.open(fin, 'r', encoding='utf-8') as f, open(fout, 'w') as out:\n",
    "    for line in f:\n",
    "        try:\n",
    "            line_num+=1\n",
    "            tokens=line.strip().lower().split(' ')\n",
    "            term_intersect=set(tokens).intersection(vocab_set)\n",
    "            if len(term_intersect) > 0:\n",
    "                token_indices=[vocab[token] for token in term_intersect] \n",
    "                #print(token_indices)\n",
    "                str1=\":1 \".join([\"{}\".format(i) for i in token_indices])+':1'\n",
    "                #print(str1)\n",
    "                out.write(\"{} {}\\n\".format(len(token_indices), str1))\n",
    "        except:\n",
    "            print(\"error\")\n",
    "            continue\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
